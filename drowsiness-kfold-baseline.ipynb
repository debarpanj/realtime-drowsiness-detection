{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc94bf5c",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mak109/cs6910_assignment2/blob/main/PART%20A/cs6910_assignment2_partA_question1_2_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af089f38",
   "metadata": {
    "id": "mdhEiMuqlUwE"
   },
   "source": [
    "# Training CNN from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec57c76c",
   "metadata": {},
   "source": [
    "## 1. Packages and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dd430fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T11:51:07.204465Z",
     "iopub.status.busy": "2023-04-26T11:51:07.203509Z",
     "iopub.status.idle": "2023-04-26T11:51:07.220425Z",
     "shell.execute_reply": "2023-04-26T11:51:07.219322Z",
     "shell.execute_reply.started": "2023-04-26T11:51:07.204425Z"
    },
    "id": "c739a581"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "from PIL import Image\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,Sequential,regularizers,optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "autotune = tf.data.AUTOTUNE\n",
    "from functools import reduce\n",
    "import random\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c327032",
   "metadata": {},
   "source": [
    "## 2. UTA-RLDD preprocessed dataset downloaded from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "505f8b0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T11:49:19.816997Z",
     "iopub.status.busy": "2023-04-26T11:49:19.816695Z",
     "iopub.status.idle": "2023-04-26T11:49:58.492794Z",
     "shell.execute_reply": "2023-04-26T11:49:58.491527Z",
     "shell.execute_reply.started": "2023-04-26T11:49:19.816968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading uta-rldd-2.zip to /kaggle/working\n",
      " 99%|█████████████████████████████████████▋| 2.38G/2.40G [00:32<00:00, 76.0MB/s]\n",
      "100%|██████████████████████████████████████| 2.40G/2.40G [00:32<00:00, 78.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!mkdir ~/.kaggle\n",
    "!touch ~/.kaggle/kaggle.json\n",
    "api_token = {\"username\":\"\",\"key\":\"\"} #Place your kaggle credentials here\n",
    "\n",
    "import json\n",
    "\n",
    "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
    "    json.dump(api_token, file)\n",
    "\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d mak1999/uta-rldd-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6dd451b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T11:51:11.053836Z",
     "iopub.status.busy": "2023-04-26T11:51:11.052781Z",
     "iopub.status.idle": "2023-04-26T11:51:25.000441Z",
     "shell.execute_reply": "2023-04-26T11:51:24.998371Z",
     "shell.execute_reply.started": "2023-04-26T11:51:11.053796Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting all the files now...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "filename = 'uta-rldd-2.zip'\n",
    "with ZipFile(filename, 'r') as z:\n",
    "    print('Extracting all the files now...')\n",
    "    z.extractall()\n",
    "    print('Done!')\n",
    "os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbb5232",
   "metadata": {},
   "source": [
    "## 3. Wandb setup for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6701a355",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T11:51:33.791237Z",
     "iopub.status.busy": "2023-04-26T11:51:33.790238Z",
     "iopub.status.idle": "2023-04-26T11:51:37.197450Z",
     "shell.execute_reply": "2023-04-26T11:51:37.196254Z",
     "shell.execute_reply.started": "2023-04-26T11:51:33.791196Z"
    },
    "id": "57958201",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "os.environ['WANDB_ENTITY'] = 'ipda526'\n",
    "os.environ['WANDB_PROJECT'] = 'baseline-drowsiness-detection'\n",
    "wandb.login(key='') #Place your wandb api key here\n",
    "from wandb.keras import WandbCallback,WandbMetricsLogger, WandbModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28e3f23",
   "metadata": {},
   "source": [
    "## 4. Model definition and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d98878aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T11:51:37.200917Z",
     "iopub.status.busy": "2023-04-26T11:51:37.200491Z",
     "iopub.status.idle": "2023-04-26T11:51:37.207814Z",
     "shell.execute_reply": "2023-04-26T11:51:37.206451Z",
     "shell.execute_reply.started": "2023-04-26T11:51:37.200870Z"
    },
    "id": "fa298523"
   },
   "outputs": [],
   "source": [
    "image_size = (256,256)\n",
    "num_layers = 4 #Number of convolution layers\n",
    "num_dense_layers = 2 #Number of dense or fully connected layers\n",
    "num_classes = 3 #0 - awake 1-drowsy 2 - low vigilant\n",
    "train_dir = 'UTA-RLDD/train'\n",
    "val_dir = 'UTA-RLDD/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "486f5aef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T11:51:37.210234Z",
     "iopub.status.busy": "2023-04-26T11:51:37.209855Z",
     "iopub.status.idle": "2023-04-26T11:51:37.222476Z",
     "shell.execute_reply": "2023-04-26T11:51:37.221305Z",
     "shell.execute_reply.started": "2023-04-26T11:51:37.210196Z"
    },
    "id": "fb1b05f7"
   },
   "outputs": [],
   "source": [
    "#CNN model hyperparameters from config\n",
    "def CNN(config):\n",
    "    model = Sequential([\n",
    "        layers.Input((image_size[0],image_size[1],3)),\n",
    "        layers.experimental.preprocessing.Rescaling(1./255)\n",
    "    ])\n",
    "    for l in range(num_layers):\n",
    "        model.add(layers.Conv2D(filters=config[\"filters_list\"][l],kernel_size=(config[\"kernel_sizes\"][l][0],config[\"kernel_sizes\"][l][1]),\n",
    "                        activation=config[\"activation\"],padding=\"same\",kernel_regularizer=regularizers.l2(config[\"weight_decay\"])))\n",
    "        if config[\"batch_normalization\"] == 'True':\n",
    "            model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(layers.Dropout(config[\"dropout_conv\"]))\n",
    "        \n",
    "    model.add(layers.Flatten())\n",
    "    for d in range(num_dense_layers-1):\n",
    "        model.add(layers.Dense(config[\"dense_layers\"][d],activation=config[\"activation\"],kernel_regularizer=regularizers.l2(config[\"weight_decay\"])))\n",
    "        model.add(layers.Dropout(config[\"dropout_dense\"]))\n",
    "    model.add(layers.Dense(config[\"dense_layers\"][num_dense_layers-1],activation=config[\"activation\"],kernel_regularizer=regularizers.l2(config[\"weight_decay\"])))\n",
    "    model.add(layers.Dense(num_classes,activation=\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af212cd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T11:51:37.228949Z",
     "iopub.status.busy": "2023-04-26T11:51:37.227983Z",
     "iopub.status.idle": "2023-04-26T11:51:37.244773Z",
     "shell.execute_reply": "2023-04-26T11:51:37.243107Z",
     "shell.execute_reply.started": "2023-04-26T11:51:37.228918Z"
    }
   },
   "outputs": [],
   "source": [
    "# tf.debugging.set_log_device_placement(True)\n",
    "tf.config.set_soft_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d988835",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T11:51:37.248212Z",
     "iopub.status.busy": "2023-04-26T11:51:37.247249Z",
     "iopub.status.idle": "2023-04-26T11:51:37.296242Z",
     "shell.execute_reply": "2023-04-26T11:51:37.294458Z",
     "shell.execute_reply.started": "2023-04-26T11:51:37.248169Z"
    },
    "id": "537a926a"
   },
   "outputs": [],
   "source": [
    "#Training goes here\n",
    "#Comment out the code related to Wandb if training is done without wandb integration\n",
    "def train(config_in=None,checkpointing=False):\n",
    "    config_ = {\n",
    "    \"kernel_sizes\" : [(5,5),(3,3),(7,7),(9,9)],\n",
    "    \"activation\" : 'elu',\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"filters_list\" : [64,32,32,16],\n",
    "    \"dense_layers\" : [256,128],\n",
    "    \"batch_normalization\": \"False\",\n",
    "    \"data_augment\": \"False\",\n",
    "    \"weight_decay\":0,\n",
    "    \"dropout_conv\":0,\n",
    "    \"dropout_dense\":0,\n",
    "    \"batch_size\":32,\n",
    "    \"epochs\":60\n",
    "    }\n",
    "    if config_in is not None:\n",
    "          config = config_in\n",
    "    else:\n",
    "          config = config_ #Default Config\n",
    "\n",
    "    '''Wandb Configs'''\n",
    "    wandb.init(config=config)\n",
    "    config = wandb.config\n",
    "    #Setting run name for better readability\n",
    "    wandb.run.name = \"nd_\"+str(len(config[\"dense_layers\"]))+\"bs_\"+str(config[\"batch_size\"])+\"ac_\"+str(config[\"activation\"])\n",
    "    #Removing the temporary train/val dir if existing\n",
    "    shutil.rmtree(train_dir,ignore_errors=True)\n",
    "    shutil.rmtree(val_dir,ignore_errors=True)\n",
    "    x = random.randint(0,4)\n",
    "    #Pick up a random fold and use it for validation and remaining other 4 folds for training\n",
    "    for i in range(5):\n",
    "        if i == x:\n",
    "            print(f'Copying fold {x+1} to val...')\n",
    "            shutil.copytree(f'UTA-RLDD/fold{x+1}','UTA-RLDD/val')\n",
    "        else:\n",
    "            print(f'Copying fold {i+1} to train...')\n",
    "            fold = f'fold{i+1}'\n",
    "            fold_path = os.path.join('UTA-RLDD', fold)\n",
    "            for subdir, dirs, files in os.walk(fold_path):\n",
    "                dest_subdir = subdir.replace(fold, 'train')\n",
    "                if not os.path.exists(dest_subdir):\n",
    "                    os.makedirs(dest_subdir)\n",
    "                for file in files:\n",
    "                    shutil.copy(os.path.join(subdir, file), os.path.join(dest_subdir, file))\n",
    "\n",
    "    print('Done!')\n",
    "    #Data Augmentation This can also be validated for better results\n",
    "    if config[\"data_augment\"] == 'True':\n",
    "        data_generator = ImageDataGenerator(\n",
    "        rotation_range=50, #random rotation between -50(clockwise) to 50(anti-clockwise) degree\n",
    "        brightness_range=(0.2,0.8), \n",
    "        zoom_range=0.3, #zoom in range from [0.7,1.3]\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        width_shift_range=0.1, #Horizontal Shifting as a ratio of width\n",
    "        height_shift_range=0.2,#Vertical Shifting as a ratio of height\n",
    "        data_format='channels_last'\n",
    "#         \n",
    "        )\n",
    "    else:\n",
    "        data_generator = ImageDataGenerator(\n",
    "            data_format='channels_last'\n",
    "        )\n",
    "    #Train set creation after conditional augmentation\n",
    "    train_generator = data_generator.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size = image_size,\n",
    "    batch_size = config['batch_size'],\n",
    "    color_mode = 'rgb',\n",
    "    class_mode = 'sparse',\n",
    "    shuffle=True,\n",
    "    seed=123\n",
    "    )\n",
    "    val_generator = ImageDataGenerator(data_format='channels_last').flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size = image_size,\n",
    "        batch_size = config['batch_size'],\n",
    "        color_mode = 'rgb',\n",
    "        class_mode = 'sparse',\n",
    "        shuffle=True,\n",
    "        seed=123\n",
    "    \n",
    "    )\n",
    "    try:\n",
    "        with tf.device('/device:GPU:0'):\n",
    "            #Building Model based on config \n",
    "            model = CNN(config)\n",
    "            #Early stopping to prevent overfitting\n",
    "            early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',patience=5,start_from_epoch=3)\n",
    "            #Compiling model \n",
    "            model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=config[\"learning_rate\"]),\n",
    "            loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy']\n",
    "            )\n",
    "            #For checkpointing default value is False\n",
    "            if checkpointing == True:\n",
    "                current_directory = os.getcwd()\n",
    "                final_directory = os.path.join(current_directory, f'models_{str(uuid.uuid1())}')\n",
    "                if not os.path.exists(final_directory):\n",
    "                    os.makedirs(final_directory)\n",
    "                checkpoint_filepath = final_directory\n",
    "                model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                  filepath=checkpoint_filepath,\n",
    "                  save_weights_only=False,\n",
    "                  monitor='val_accuracy',\n",
    "                  mode='max',\n",
    "                  save_best_only=True)\n",
    "                  #Fitting Model\n",
    "                history = model.fit(train_generator,\n",
    "                  validation_data=val_generator,\n",
    "                  epochs=config[\"epochs\"],\n",
    "                  verbose=1,\n",
    "                  # callbacks = [WandbCallback()] #Used with wandb\n",
    "                  callbacks = [early_stop_callback,model_checkpoint_callback] #Custom callback for checkpointing\n",
    "                  )\n",
    "            else:\n",
    "                history = model.fit(train_generator,\n",
    "                  validation_data=val_generator,\n",
    "                  epochs=config[\"epochs\"],\n",
    "                  verbose=1,#WandbMetricsLogger(log_freq=\"epoch\"),\n",
    "                  callbacks = [early_stop_callback,WandbCallback(monitor='val_accuracy',mode='auto'),\n",
    "                               WandbModelCheckpoint(filepath=\"models\",monitor='val_accuracy',verbose=1,save_freq='epoch',mode='max')] #Used with wandb\n",
    "                  )\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "          print(e)\n",
    "    wandb.finish()\n",
    "    shutil.rmtree(train_dir,ignore_errors=True)\n",
    "    shutil.rmtree(val_dir,ignore_errors=True)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed07e59",
   "metadata": {},
   "source": [
    "## Standalone training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3520c91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T11:51:43.208212Z",
     "iopub.status.busy": "2023-04-26T11:51:43.207511Z",
     "iopub.status.idle": "2023-04-26T11:51:43.212807Z",
     "shell.execute_reply": "2023-04-26T11:51:43.211359Z",
     "shell.execute_reply.started": "2023-04-26T11:51:43.208178Z"
    }
   },
   "outputs": [],
   "source": [
    "history = train()\n",
    "#Visualization part\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "# plt.savefig('metrics.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af16e6b",
   "metadata": {},
   "source": [
    "## Wandb integration for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed62111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T11:54:07.257275Z",
     "iopub.status.busy": "2023-04-26T11:54:07.256422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: cu69pkll\n",
      "Sweep URL: https://wandb.ai/ipda526/baseline-drowsiness-detection/sweeps/cu69pkll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xc3l8g28 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: elu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_normalization: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augment: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_layers: [16, 64]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_conv: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_dense: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilters_list: [128, 256, 512, 64]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_sizes: [[5, 5], [3, 3], [7, 7], [9, 9]]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmayukhdas04\u001b[0m (\u001b[33mipda526\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20230426_115415-xc3l8g28</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ipda526/baseline-drowsiness-detection/runs/xc3l8g28' target=\"_blank\">swift-sweep-1</a></strong> to <a href='https://wandb.ai/ipda526/baseline-drowsiness-detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ipda526/baseline-drowsiness-detection/sweeps/cu69pkll' target=\"_blank\">https://wandb.ai/ipda526/baseline-drowsiness-detection/sweeps/cu69pkll</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ipda526/baseline-drowsiness-detection' target=\"_blank\">https://wandb.ai/ipda526/baseline-drowsiness-detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ipda526/baseline-drowsiness-detection/sweeps/cu69pkll' target=\"_blank\">https://wandb.ai/ipda526/baseline-drowsiness-detection/sweeps/cu69pkll</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ipda526/baseline-drowsiness-detection/runs/xc3l8g28' target=\"_blank\">https://wandb.ai/ipda526/baseline-drowsiness-detection/runs/xc3l8g28</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying fold 1 to train...\n",
      "Copying fold 2 to train...\n",
      "Copying fold 3 to train...\n",
      "Copying fold 4 to val...\n",
      "Copying fold 5 to train...\n",
      "Done!\n",
      "Found 7950 images belonging to 3 classes.\n",
      "Found 2196 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/backend.py:5586: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits, \"Softmax\", \"sparse_categorical_crossentropy\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/249 [==============================] - ETA: 0s - loss: 0.2975 - accuracy: 0.8819"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/kaggle/working/wandb/run-20230426_115415-xc3l8g28/files/model-best)... Done. 0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./models)... Done. 0.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/249 [==============================] - 262s 1s/step - loss: 0.2975 - accuracy: 0.8819 - val_loss: 2.5434 - val_accuracy: 0.4740\n",
      "Epoch 2/60\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9904\n",
      "Epoch 2: saving model to models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./models)... Done. 0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/249 [==============================] - 246s 986ms/step - loss: 0.0335 - accuracy: 0.9904 - val_loss: 3.0988 - val_accuracy: 0.4180\n",
      "Epoch 3/60\n",
      "249/249 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 0.9996"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/kaggle/working/wandb/run-20230426_115415-xc3l8g28/files/model-best)... Done. 0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: saving model to models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./models)... Done. 0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/249 [==============================] - 249s 998ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 2.8883 - val_accuracy: 0.5342\n",
      "Epoch 4/60\n",
      "183/249 [=====================>........] - ETA: 50s - loss: 9.1258e-04 - accuracy: 1.0000\n",
      "Epoch 4: saving model to models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./models)... Done. 0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/249 [==============================] - 243s 975ms/step - loss: 8.4787e-04 - accuracy: 1.0000 - val_loss: 3.4707 - val_accuracy: 0.4750\n",
      "Epoch 5/60\n",
      "249/249 [==============================] - ETA: 0s - loss: 1.6298e-04 - accuracy: 1.0000\n",
      "Epoch 5: saving model to models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./models)... Done. 0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/249 [==============================] - 241s 966ms/step - loss: 1.6298e-04 - accuracy: 1.0000 - val_loss: 3.4518 - val_accuracy: 0.4754\n",
      "Epoch 6/60\n",
      "249/249 [==============================] - ETA: 0s - loss: 6.1166e-05 - accuracy: 1.0000\n",
      "Epoch 6: saving model to models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./models)... Done. 0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/249 [==============================] - 241s 968ms/step - loss: 6.1166e-05 - accuracy: 1.0000 - val_loss: 3.5498 - val_accuracy: 0.4754\n",
      "Epoch 7/60\n",
      "249/249 [==============================] - ETA: 0s - loss: 4.0869e-05 - accuracy: 1.0000\n",
      "Epoch 7: saving model to models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./models)... Done. 0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/249 [==============================] - 242s 971ms/step - loss: 4.0869e-05 - accuracy: 1.0000 - val_loss: 3.5882 - val_accuracy: 0.4763\n",
      "Epoch 8/60\n",
      "166/249 [===================>..........] - ETA: 1:02 - loss: 3.0869e-05 - accuracy: 1.0000\n",
      "Epoch 8: saving model to models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./models)... Done. 0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/249 [==============================] - 239s 960ms/step - loss: 2.9780e-05 - accuracy: 1.0000 - val_loss: 3.6552 - val_accuracy: 0.4745\n",
      "Epoch 9/60\n",
      "  4/249 [..............................] - ETA: 3:00 - loss: 9.1260e-05 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "'''Wandb Sweeps '''\n",
    "#Sweep configuration for runs\n",
    "sweep_config = {\n",
    "  \"name\" : \"best-sweep-baseline-folds\"+str(uuid.uuid1()),\n",
    "  \"method\" : \"bayes\",\n",
    "  \"metric\" : {\n",
    "      \"name\" : \"val_accuracy\",\n",
    "      \"goal\" : \"maximize\"\n",
    "  },\n",
    "  \"parameters\" : {\n",
    "    \"epochs\" : {\n",
    "      \"values\" : [20,30,40,50,60]\n",
    "    },\n",
    "    \"learning_rate\" :{\n",
    "      \"values\" : [1e-2,1e-3,1e-4]\n",
    "    },\n",
    "    \"kernel_sizes\":{\n",
    "        \"values\" : [[(3,3),(3,3),(3,3),(3,3)],\n",
    "                    [(3,3),(9,9),(10,10),(12,12)],\n",
    "                    [(3,3),(3,3),(5,5),(7,7)],\n",
    "                    [(11,11),(11,11),(7,7),(5,5)],\n",
    "                    [(5,5),(3,3),(7,7),(9,9)],\n",
    "                    [(5,5),(5,5),(5,5),(5,5)]]\n",
    "    },\n",
    "    \"filters_list\":{\n",
    "        \"values\" : [[32,32,32,32],[256,128,64,32],[32,64,64,128],[128,256,512,64],[64,32,64,32]]\n",
    "    },\n",
    "    \"weight_decay\":{\n",
    "      \"values\": [0,0.0005,0.005,0.05]  \n",
    "    },\n",
    "    \"data_augment\":{\n",
    "        \"values\": [\"True\",\"False\"]\n",
    "    },\n",
    "    \"batch_size\":{\n",
    "        \"values\":[16,32,64]\n",
    "    },\n",
    "    \"activation\":{\n",
    "        \"values\": [\"relu\",\"elu\",\"swish\",\"gelu\"]\n",
    "    },\n",
    "      \"dropout_dense\":{\n",
    "          \"values\":[0.0,0.4,0.6]\n",
    "      },\n",
    "      \"dropout_conv\":{\n",
    "          \"values\":[0.0,0.2,0.3,0.4]\n",
    "      },\n",
    "      \"dense_layers\":{\n",
    "          \"values\":[[32,64],[64,32],[256,256],[16,64],[512,256]]\n",
    "      },\n",
    "      \"batch_normalization\":{\n",
    "          \"values\":[\"True\",\"False\"]\n",
    "      }\n",
    "  }\n",
    "}\n",
    "sweep_id=wandb.sweep(sweep_config,entity=\"ipda526\",project=\"baseline-drowsiness-detection\")\n",
    "wandb.agent(sweep_id, function=train, count=10) #10 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda298c",
   "metadata": {
    "id": "kNQX6n7Ij_mF"
   },
   "outputs": [],
   "source": [
    "'''This section is used for loading the models saved with datetime when checkpointing is True'''\n",
    "# #This can be used when checkpointing is set to True and models are saved in model directory with proper name in the current working directory\n",
    "# model_dir = 'models_<unique id>' #model director name goes here\n",
    "# new_model = tf.keras.models.load_model(model_dir)\n",
    "# # Check its architecture\n",
    "# new_model.summary()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "cs6910_assignment2_partA_question1_2_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
